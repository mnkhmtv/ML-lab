{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“Œ Machine Learning Assignment 1 - Instructions & Guidelines\n",
    "\n",
    "### **ðŸ“ General Guidelines**\n",
    "Welcome to Machine Learning Assignment 1! This assignment will test your understanding of **regression and classification models**, including **data preprocessing, hyperparameter tuning, and model evaluation**.\n",
    "\n",
    "Follow the instructions carefully, and ensure your implementation is **correct, well-structured, and efficient**.\n",
    "\n",
    "ðŸ”¹ **Submission Format:**  \n",
    "- Your submission **must be a single Jupyter Notebook (.ipynb)** file.  \n",
    "- **File Naming Convention:**  \n",
    "  - Use **your university email as the filename**, e.g.,  \n",
    "    ```\n",
    "    j.doe@innopolis.university.ipynb\n",
    "    ```\n",
    "  - **Do NOT modify this format**, or your submission may not be graded.\n",
    "\n",
    "ðŸ”¹ **Assignment Breakdown:**\n",
    "| Task | Description | Points |\n",
    "|------|------------|--------|\n",
    "| **Task 1.1** | Linear Regression | 20 |\n",
    "| **Task 1.2** | Polynomial Regression | 20 |\n",
    "| **Task 2.1** | Data Preprocessing | 15 |\n",
    "| **Task 2.2** | Model Comparison | 45 |\n",
    "| **Total** | - | **100** |\n",
    "\n",
    "---\n",
    "\n",
    "### **ðŸ“‚ Dataset & Assumptions**\n",
    "The dataset files are stored in the `datasets/` folder.  \n",
    "- **Regression Dataset:** `datasets/task1_data.csv`\n",
    "- **Classification Dataset:** `datasets/pokemon_modified.csv`\n",
    "\n",
    "Each dataset is structured as follows:\n",
    "\n",
    "ðŸ”¹ **`task1_data.csv` (for regression tasks)**  \n",
    "- Contains `X_train`, `y_train`, `X_test`, and `y_test`.  \n",
    "- The goal is to fit **linear and polynomial regression models** and evaluate their performance.  \n",
    "\n",
    "ðŸ”¹ **`pokemon_modified.csv` (for classification tasks)**  \n",
    "- Contains PokÃ©mon attributes, with `is_legendary` as the **binary target variable (0 or 1)**.  \n",
    "- Some features contain **missing values** and **categorical variables**, requiring preprocessing.\n",
    "\n",
    "---\n",
    "\n",
    "### **ðŸš€ How to Approach the Assignment**\n",
    "1. **Start with Regression (Task 1)**\n",
    "   - Implement **linear regression** and **polynomial regression**.\n",
    "   - Use **GridSearchCV** for polynomial regression to find the best degree.\n",
    "   - Evaluate using **MSE, RMSE, MAE, and RÂ² Score**.\n",
    "\n",
    "2. **Move to Data Preprocessing (Task 2.1)**\n",
    "   - Load and clean the PokÃ©mon dataset.\n",
    "   - Handle **missing values** correctly.\n",
    "   - Encode categorical variables properly.\n",
    "   - Ensure **no data leakage** when doing the preprocessing.\n",
    "\n",
    "3. **Train and Evaluate Classification Models (Task 2.2)**\n",
    "   - Train **Logistic Regression, KNN, and Naive Bayes**.\n",
    "   - Use **GridSearchCV** for hyperparameter tuning.\n",
    "   - Evaluate models using **Accuracy, Precision, Recall, and F1-score**.\n",
    "\n",
    "---\n",
    "\n",
    "### **ðŸ“Œ Grading & Evaluation**\n",
    "- Your notebook will be **autograded**, so ensure:\n",
    "  - Your function names **exactly match** the given specifications.\n",
    "  - Your output format matches the expected results.\n",
    "- Partial credit will be given where applicable.\n",
    "\n",
    "ðŸ”¹ **Need Help?**  \n",
    "- If you have any questions, refer to the **assignment markdown instructions** in each task before asking for clarifications.\n",
    "- You can post your question on this [Google sheet](https://docs.google.com/spreadsheets/d/1oyrqXDjT2CeGYx12aZhZ-oDKcQQ-PCgT91wHPhTlBCY/edit?usp=sharing)\n",
    "\n",
    "ðŸš€ **Good luck! Happy coding!** ðŸŽ¯"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FAQ\n",
    "\n",
    "**1) Should we include the lines to import the libraries?**\n",
    "\n",
    "- **Answer:**  \n",
    "  It doesn't matter if you include extra import lines, as the grader will only call the specified functions.\n",
    "\n",
    "**2) Is it okay to submit my file with code outside of the functions?**\n",
    "\n",
    "- **Answer:**  \n",
    "  Yes, you can include additional code outside of the functions as long as the entire script runs correctly when converted to a `.py` file.\n",
    "\n",
    "**Important Clarification:**\n",
    "\n",
    "- The grader will first convert the Jupyter Notebook (.ipynb) into a Python file (.py) and then run it.\n",
    "- **Note:** Please do not include any commands like `!pip install numpy` because they may break the conversion process and therefore the submission will not be graded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Linear and Polynomial Regression (30 Points)\n",
    "\n",
    "### Task 1.1 - Linear Regression (15 Points)\n",
    "#### **Instructions**\n",
    "1. Load the dataset from **`datasets/task1_data.csv`**.\n",
    "2. Extract training and testing data from the following columns:\n",
    "   - `\"X_train\"`: Training feature values.\n",
    "   - `\"y_train\"`: Training target values.\n",
    "   - `\"X_test\"`: Testing feature values.\n",
    "   - `\"y_test\"`: Testing target values.\n",
    "3. Train a **linear regression model** on `X_train` and `y_train`.\n",
    "4. Use the trained model to predict `y_test` values.\n",
    "5. Compute and return the following **evaluation metrics** as a dictionary:\n",
    "   - **Mean Squared Error (MSE)**\n",
    "   - **Root Mean Squared Error (RMSE)**\n",
    "   - **Mean Absolute Error (MAE)**\n",
    "   - **RÂ² Score**\n",
    "6. The function signature should match:\n",
    "   ```python\n",
    "   def task1_linear_regression() -> Dict[str, float]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please do not use any other libraries except for the ones imported below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Library Imports\n",
    "import os\n",
    "import importlib.util\n",
    "import nbformat\n",
    "from tempfile import NamedTemporaryFile\n",
    "from typing import Tuple, Dict\n",
    "\n",
    "# Third-Party Library Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nbconvert import PythonExporter\n",
    "\n",
    "# Scikit-Learn Imports\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, PolynomialFeatures, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
    "                             mean_squared_error, mean_absolute_error, r2_score)\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task1_linear_regression() -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Performs linear regression on a predefined dataset and returns performance metrics.\n",
    "\n",
    "    **Dataset Assumption:**\n",
    "    - The dataset is located at `\"datasets/task1_data.csv\"`.\n",
    "    - It should contain the following columns:\n",
    "      - `\"X_train\"`: Training feature values (numerical).\n",
    "      - `\"y_train\"`: Training target values.\n",
    "      - `\"X_test\"`: Testing feature values (numerical).\n",
    "      - `\"y_test\"`: Testing target values.\n",
    "\n",
    "    **Process:**\n",
    "    1. Load the dataset from `\"datasets/task1_data.csv\"`.\n",
    "    2. Extract training and testing data.\n",
    "    3. Train a linear regression model on `X_train, y_train`.\n",
    "    4. Use the trained model to predict `y_test` values.\n",
    "    5. Compute evaluation metrics: **MSE, RMSE, MAE, RÂ² Score**.\n",
    "\n",
    "    **Output (Dictionary with Regression Metrics):**\n",
    "    ```python\n",
    "    {\n",
    "        \"MSE\": <Mean Squared Error>,\n",
    "        \"RMSE\": <Root Mean Squared Error>,\n",
    "        \"MAE\": <Mean Absolute Error>,\n",
    "        \"R2\": <RÂ² Score>\n",
    "    }\n",
    "    ```\n",
    "    \"\"\"\n",
    "\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.2 - Polynomial Regression (15 Points)\n",
    "\n",
    "#### **Instructions**\n",
    "1. Load the dataset from **`datasets/task1_data.csv`**.\n",
    "2. Extract training and testing data from the following columns:\n",
    "   - `\"X_train\"`: Training feature values.\n",
    "   - `\"y_train\"`: Training target values.\n",
    "   - `\"X_test\"`: Testing feature values.\n",
    "   - `\"y_test\"`: Testing target values.\n",
    "3. Define a **pipeline** that includes:\n",
    "   - **Polynomial feature transformation** (degree range: **2 to 10**).\n",
    "   - **Linear regression model**.\n",
    "4. Use **GridSearchCV** with **8-fold cross-validation** to determine the best polynomial degree.\n",
    "5. Train the model with the best polynomial degree and **evaluate it on the test set**.\n",
    "6. Compute and return the following results as a dictionary:\n",
    "   - **Best polynomial degree** (`best_degree`)\n",
    "   - **Mean Squared Error (MSE)**\n",
    "\n",
    "#### **Function Signature**\n",
    "```python\n",
    "def task1_polynomial_regression() -> Dict[str, float]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task1_polynomial_regression() -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Performs polynomial regression using GridSearchCV to find the best polynomial degree.\n",
    "\n",
    "\n",
    "    **Process:**\n",
    "    1. Load the dataset and extract `X_train, y_train, X_test, y_test`.\n",
    "    2. Define a **pipeline** with polynomial feature transformation and linear regression.\n",
    "    3. Use **GridSearchCV** (with 8-fold cross-validation) to determine the best polynomial degree (range: **2 to 10**).\n",
    "    4. Train the best polynomial regression model and evaluate its performance.\n",
    "    5. Compute and return:\n",
    "       - **Best polynomial degree (`best_degree`)**\n",
    "       - **Mean Squared Error (MSE)**\n",
    "\n",
    "     **Expected Output:**\n",
    "    ```\n",
    "    {\n",
    "        \"best_degree\": <Optimal Polynomial Degree>,\n",
    "        \"MSE\": <Mean Squared Error>\n",
    "    }\n",
    "    ```\n",
    "    \"\"\"\n",
    "\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Classification with Data Preprocessing (70 Points)\n",
    "\n",
    "### Task 2.1 - Data Preprocessing (30 Points)\n",
    "\n",
    "#### **Instructions**\n",
    "1. Load the dataset from **`datasets/pokemon_modified.csv`**.\n",
    "2. Look at the data and study the provided features\n",
    "3. Remove the **two redundant features**\n",
    "4. Handle **missing values**:\n",
    "   - Use **mean imputation** for **\"height_m\"** and **\"weight_kg\"**.\n",
    "   - Use **median imputation** for **\"percentage_male\"**.\n",
    "5. Perform **one-hot encoding** for the categorical column **\"type1\"**.\n",
    "6. Ensure the **target variable** (`\"is_legendary\"`) is present.\n",
    "7. **Split the data into training and testing sets** (`80%-20%` split). Is it balanced?\n",
    "8. **Apply feature scaling** using **StandardScaler** or **MinMaxScaler**.\n",
    "9. Return the following:\n",
    "   - `X_train_scaled`: Processed training features.\n",
    "   - `X_test_scaled`: Processed testing features.\n",
    "   - `y_train`: Training labels.\n",
    "   - `y_test`: Testing labels.\n",
    "\n",
    "#### **Function Signature**\n",
    "```python\n",
    "def task2_preprocessing() -> Tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task2_preprocessing() -> Tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series]:\n",
    "    \"\"\"\n",
    "    Preprocesses the PokÃ©mon dataset by handling missing values, encoding categorical data, \n",
    "    and applying feature scaling before returning train-test splits, ensuring class balance.\n",
    "\n",
    "    **Dataset Assumption:**\n",
    "    - The dataset is located at `\"datasets/pokemon_modified.csv\"`.\n",
    "\n",
    "    **Process:**\n",
    "    1. Load the dataset and remove redundant columns.\n",
    "    2. Handle missing values:\n",
    "       - Mean imputation for **\"height_m\"** and **\"weight_kg\"**.\n",
    "       - Median imputation for **\"percentage_male\"**.\n",
    "    3. Perform **one-hot encoding** on `\"type1\"`.\n",
    "    4. Ensure **\"is_legendary\"** is present as the target variable.\n",
    "    5. Split the dataset into **80% training, 20% testing** using **stratification** to maintain class balance.\n",
    "    6. Apply feature scaling (**StandardScaler**).\n",
    "    7. Return the preprocessed train-test splits.\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.2 - Model Comparison (40 Points)\n",
    "\n",
    "#### **Instructions**\n",
    "1. **Train three classification models** on the preprocessed dataset:\n",
    "   - **Logistic Regression**\n",
    "   - **K-Nearest Neighbors (KNN)**\n",
    "   - **Gaussian Naive Bayes (GNB)**\n",
    "2. Use **GridSearchCV** for **hyperparameter tuning** on:\n",
    "   - **Logistic Regression**: Regularization strength (`C`) and penalty (`l1`, `l2`).\n",
    "   - **KNN**: Number of neighbors (`n_neighbors`), weight function, and distance metric.\n",
    "3. Train each model on the **training set** and evaluate on the **test set**.\n",
    "4. Compute the following **evaluation metrics**:\n",
    "   - **Accuracy**\n",
    "   - **Precision**\n",
    "   - **Recall**\n",
    "   - **F1 Score**\n",
    "5. Return a dictionary containing the evaluation metrics for each model.\n",
    "\n",
    "#### **Function Signature**\n",
    "```python\n",
    "def task2_model_comparison() -> Dict[str, Dict[str, float]]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task2_model_comparison() -> Dict[str, Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Trains and evaluates three classification models using GridSearchCV for hyperparameter tuning.\n",
    "\n",
    "    **Dataset Assumption:**\n",
    "    - The preprocessed dataset is obtained from `task2_preprocessing()`, which returns:\n",
    "      - `X_train`: Training features (scaled)\n",
    "      - `X_test`: Testing features (scaled)\n",
    "      - `y_train`: Training labels\n",
    "      - `y_test`: Testing labels\n",
    "\n",
    "    **Process:**\n",
    "    1. Load the preprocessed dataset from `task2_preprocessing()`.\n",
    "    2. Train the following models:\n",
    "       - **Logistic Regression** (Hyperparameters: `C`, `penalty`, `solver`).\n",
    "       - **K-Nearest Neighbors (KNN)** (Hyperparameters: `n_neighbors`, `weights`, `metric`).\n",
    "       - **Gaussian Naive Bayes** (No hyperparameter tuning required).\n",
    "    3. Evaluate the models using the following metrics:\n",
    "       - **Accuracy**\n",
    "       - **Precision**\n",
    "       - **Recall**\n",
    "       - **F1 Score**\n",
    "    4. Return a dictionary with model names as keys and evaluation metrics as values.\n",
    "\n",
    "    **Expected Output:**\n",
    "    ```python\n",
    "    {\n",
    "        \"Logistic Regression\": {\"accuracy\": <float>, \"precision\": <float>, \"recall\": <float>, \"f1_score\": <float>},\n",
    "        \"KNN\": {\"accuracy\": <float>, \"precision\": <float>, \"recall\": <float>, \"f1_score\": <float>},\n",
    "        \"Naive Bayes\": {\"accuracy\": <float>, \"precision\": <float>, \"recall\": <float>, \"f1_score\": <float>}\n",
    "    }\n",
    "    ```\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    return "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
