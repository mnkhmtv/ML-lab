{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YQPaHbIjl01G"
   },
   "source": [
    "## Lab-3: Logistic Regression and Gradient descent\n",
    "-----\n",
    "\n",
    "## Introduction \n",
    "##### In this lab, we will discuss Logistic Regression (LR). This is a very important problem in machine learning and is applied to solve a variety of real life tasks. For example, financial services such as banks can rely on historical data of customers' credit record for loan approval by humans to automate the decision making process. A logistic regression model can be trained on these data, and then used to predict loan acceptance for future applicants. In particalar in this lab, you will apply Logistic Regression to predict survival on Titanic and learn to analyze classification results. \n",
    "\n",
    "### Learning outcomes:\n",
    "1. Compare classification problem (data) with regression problem (data)\n",
    "2. Understand the core concepts of LR (sigmoid function and loss function)\n",
    "3. Manual Feature Selection (are all the features relevant for the task?)\n",
    "4. Perform data pre-processing and preparation. Which pre-precessing technique should be aplied and why? \n",
    "5. Implement Logistic Regression with sklearn\n",
    "6. Interprete the results of logistic regression (accuracy, confusion matrix, recall, precision, f1-score)\n",
    "7. Strength and weakness of logistic regression\n",
    "8. LR with gradient descent \n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DJE95f4yGU_i"
   },
   "source": [
    "### Concepts Visualization\n",
    "\n",
    "##### Here are the main equations behind Logistic Regression:\n",
    "\n",
    "\n",
    "##### Given a data point $(x_i, y_i)$ where $x_i=\\{x_i^1,x_i^2, x_i^3,...,x_i^n\\}$;  $x_i^j$ represents the feature $j$ and $y_i$ the associated class label.\n",
    "\n",
    "#### Considering the hypothesis function $f(x_i) = \\beta_0 + \\beta_1 x_i^1 + + \\beta_2 x_i^2 + ... + \\beta_n x_i^n$ the output the LR model is the sigmoid function applied to $f(x_i)$, i.e., \n",
    "\n",
    "\n",
    "##### $\\hat p(x_i) = \\frac{e^{f(x_i)}}{1+e^{f(x_i)}} = \\frac{1}{1+e^{-f(x_i)}}$\n",
    "\n",
    "#### Components of the loss function\n",
    "\n",
    "\n",
    "##### Given the predicted $\\hat{p}(x_i)$ of the $x_i$ The loss of classifying the data point  is computed as follow:\n",
    " \n",
    "$L(\\hat{p}(x_i), y_i) = \\left\\{\\begin{matrix}\n",
    "-\\log (\\hat{p}(x_i)) && y_i = 1\\\\ \n",
    "-\\log (1 -\\hat{p}(x_i)) && y_i = 0 \n",
    "\\end{matrix}\\right.$\n",
    "\n",
    "$L(\\hat{p}(x_i), y_i) = -y_i\\log (\\hat{p}(x_i)) - (1 - y)_i\\log (1 -\\hat{p}(x_i))$\n",
    " \n",
    "\n",
    "#### The goal of the LR model is to find the values of the parameters $\\{\\beta_i\\}_{i=0}^{n}$ such that the average loss $L$ on the training dataset is minimized\n",
    "\n",
    "$\\hat y = \\left\\{\\begin{matrix}\n",
    "1 && \\hat p(x) > threshold\\\\ \n",
    "0 && otherwise \n",
    "\\end{matrix}\\right.$\n",
    "\n",
    " \n",
    "### Open Questions\n",
    "1. What problem does Logistic Regression solve?  \n",
    "2. What is the output of Logistic Regression model? \n",
    "3. Why can't we use average MSE as a loss function (as we did in Linear Regression)?   \n",
    " \n",
    "Let's now see how the shape of $\\hat p(x)$ depend on its parameters for a single feature: $\\beta_0 + \\beta_1 x_i$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8mwZdElFli7v"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "colab_type": "code",
    "id": "zibEd5lsI2Xn",
    "outputId": "fdecdabc-0bba-4695-decb-dd2d1c176b3c"
   },
   "outputs": [],
   "source": [
    "x = np.arange(-10, 10, 0.01)\n",
    "\n",
    "def plot(b0, b1):\n",
    "    p = 1 / (1 + np.exp(-(b0 + b1 * x)))\n",
    "    label = \"b0 = {}, b1 = {}\".format(b0, b1)\n",
    "    plt.plot(x, p, label=label)\n",
    "\n",
    "plot(0,1)\n",
    "plot(0,2)\n",
    "plot(0,3)\n",
    "plot(5,1)\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.title('Sigmoid function')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_rqF7CozNK1F"
   },
   "source": [
    "What is controlled by parameters b0, b1?  \n",
    "<br><br> \n",
    "Let's now visualize the components of the Loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "colab_type": "code",
    "id": "0nQEsc6XJbDp",
    "outputId": "68e2d1d0-b18c-4d8f-e2b7-1c78cbb81824",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = np.arange(0.001, 1, 0.001)\n",
    "y1 = -np.log(x)\n",
    "y0 = -np.log(1-x)\n",
    "\n",
    "plt.plot(x,y1, label=\"if y = 1\")\n",
    "plt.plot(x,y0, label=\"if y = 0\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the benefits of using logarithms in the loss function?   <br><br>\n",
    "\n",
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('titanic.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing name column\n",
    "data = data.drop(['name'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do we know about the output class distribution? What percentage of people had survived? Is it important to have the same distribution in test and train sets? Why?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1 .\n",
    "\n",
    "Find and print the proporion of the positive (survived) class in the dataset, then split data to train and test sets preserving that proportion. Finally, as a check, calculate and print proportions in resulting sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# find and print the proportion of positive samples in data\n",
    "print('% of positive samples in whole data:', 0)\n",
    "\n",
    "# split data DataFrame into label column (\"survived\") and the rest columns as features\n",
    "data_label = None\n",
    "data_feature = None\n",
    "\n",
    "# split by train_test_split\n",
    "x_train, x_test, y_train, y_test = None\n",
    "\n",
    "# find and print the proportion of positive samples in train and test sets, make sure they are approx same\n",
    "print('% of positive samples in train set:', 0)\n",
    "print('% of positive samples in test set:', 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data preprocessing\n",
    "\n",
    "... is done for you! But you are free to change it if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# imputing missing values\n",
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "imputer.fit(x_train)\n",
    "x_train = pd.DataFrame(imputer.transform(x_train), columns=x_train.columns)\n",
    "x_test = pd.DataFrame(imputer.transform(x_test), columns=x_test.columns)\n",
    "\n",
    "# one-hot-encode categorical features\n",
    "def ohe_new_features(df, features_name, encoder):\n",
    "    new_feats = encoder.transform(df[features_name])\n",
    "    # create dataframe from encoded features with named columns\n",
    "    new_cols = pd.DataFrame(new_feats, columns=encoder.get_feature_names_out(features_name))\n",
    "    new_df = pd.concat([df, new_cols], axis=1)    \n",
    "    new_df.drop(features_name, axis=1, inplace=True)\n",
    "    return new_df\n",
    "\n",
    "encoder = OneHotEncoder(sparse=False, drop='first')\n",
    "f_names = ['sex', 'embarked']\n",
    "encoder.fit(x_train[f_names])\n",
    "x_train = ohe_new_features(x_train, f_names, encoder)\n",
    "x_test = ohe_new_features(x_test, f_names, encoder)\n",
    "\n",
    "# feature scaling\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(x_train)\n",
    "x_train = pd.DataFrame(scaler.transform(x_train), columns=x_train.columns)\n",
    "x_test = pd.DataFrame(scaler.transform(x_test), columns=x_test.columns)\n",
    "x_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "\n",
    "Now we are ready to see Logistic Regression in practice. \n",
    "\n",
    "#### Task 2. \n",
    "Fit Logistic Regression and output Accuracy, Precision,  Recall and F1 scores on test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "# fit Logistic Regression\n",
    "clf = None\n",
    "\n",
    "# caclulate and print metrics\n",
    "print(f'Testing accuracy = {metrics.accuracy_score(y_test, y_test_pred)}')\n",
    "print(f'Testing precision = {metrics.precision_score(y_test, y_test_pred)}')\n",
    "print(f'Testing recall = {metrics.recall_score(y_test, y_test_pred)}')\n",
    "print(f'Testing F1-score = {metrics.f1_score(y_test, y_test_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question\n",
    "\n",
    "Based on the metrics, is our model good enough? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about Logistic Regression Coefficients? Can we interpret them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(*[a for a in zip(list(x_train.columns), clf.coef_[0])], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we choose a threshold? Let's try different ones and see their effect on metrics.\n",
    "\n",
    "#### Task 3. \n",
    "\n",
    "Calculate Accuracy, Precision, and Recall values for each of the given threshold values and plot them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thresholds for LR \n",
    "thresholds = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "\n",
    "# Probabilities of each sample from x_test\n",
    "# Hint: LR has a function predict_proba\n",
    "pred_proba = ???\n",
    "\n",
    "# Accuracy, precision, recall\n",
    "results = [[],[],[]]\n",
    "for i in thresholds:\n",
    "    # Write your code here: if the element from pred_proba > threshold,\n",
    "    # add 1 to y_test_pred_thr, otherwise add 0\n",
    "    y_test_pred_thr = ???\n",
    "    # Compare y_test_pred_thr with y_test by metrics and add them to results\n",
    "    pass\n",
    "    \n",
    "plt.plot(thresholds, results[0], label = 'accuracy')   \n",
    "plt.plot(thresholds, results[1], label = 'precision')   \n",
    "plt.plot(thresholds, results[2], label = 'recall')\n",
    "plt.title('Threshold Selection')\n",
    "plt.xlabel('threshold')\n",
    "plt.ylabel('score')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix\n",
    "\n",
    "1. Why one may need confusion matrix? \n",
    "2. What new information does it show? \n",
    "\n",
    "<table><tr><td><img src='https://rasbt.github.io/mlxtend/user_guide/evaluate/confusion_matrix_files/confusion_matrix_1.png'></td></tr></table>\n",
    "\n",
    "#### In multiclass (more than 2) case: \n",
    "<img align='center' src='https://www.researchgate.net/profile/Emanuele_Principi/publication/324226324/figure/fig2/AS:612091408941056@1522945372985/Normalized-confusion-matrix-of-best-performing-models-on-devel-subset-a-SVM.png' style='width: 350px;'>\n",
    "\n",
    "where V, O, T, E - names of the classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "titanic_true_labels = np.array(y_test)\n",
    "titanic_pred_labels = y_test_pred\n",
    "\n",
    "print(f'Confusion Matrix for Titanic:  \\n {confusion_matrix(titanic_true_labels, titanic_pred_labels)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In seaborn this matrix is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "ax= plt.subplot()\n",
    "sns.heatmap(confusion_matrix(titanic_true_labels, titanic_pred_labels), annot=True, fmt='g', ax=ax)  #annot=True to annotate cells, ftm='g' to disable scientific notation\n",
    "\n",
    "# labels, title and ticks\n",
    "ax.set_xlabel('Predicted labels')\n",
    "ax.set_ylabel('Actual labels')\n",
    "ax.set_title('Confusion Matrix')\n",
    "ax.xaxis.set_ticklabels(['Not Survived', 'Survived']); ax.yaxis.set_ticklabels(['Not Survived', 'Survived'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question\n",
    "\n",
    "Which class (survived, not survived) is more precisely predicted?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center> Gradient Descent: Losgistic regression with Numpy </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_blobs\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, labels = make_blobs(n_samples=1000, centers=2, n_features=2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = {0:\"red\", 1:'blue'}\n",
    "label_values = [0, 1]\n",
    "\n",
    "for i in label_values:\n",
    "    idx = np.where(labels == i)\n",
    "    plt.scatter(data[idx, 0], data[idx, 1], c=colors[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Logistic Sigmoid Function $\\sigma(z)$\n",
    "\n",
    "$$ \\sigma(z) = \\frac{1}{1+e^{-z}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x): \n",
    "    # return the sigmoid   \n",
    "    return 1/ (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the Cost Function $J(\\theta)$ and Gradient\n",
    "\n",
    "The objective of logistic regression is to minimize the cost function\n",
    "\n",
    "$$J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^{m} [ y^{(i)}log(h_{\\theta}(x^{(i)})) + (1 - y^{(i)})log(1 - (h_{\\theta}(x^{(i)}))]$$\n",
    "\n",
    "where the gradient of the cost function is given by\n",
    "\n",
    "$$ \\frac{\\partial J(\\theta)}{\\partial \\theta_j} = \\frac{1}{m} \\sum_{i=1}^{m} (h_{\\theta}(x^{(i)}) - y^{(i)})x_j^{(i)}$$\n",
    "\n",
    "In vector form:\n",
    "$$ \\frac{\\partial J(\\theta)}{\\partial \\theta} = \\frac{1}{m} \\textbf{X}^T * (h_{\\theta}(\\textbf{X}) - \\textbf{y}) $$\n",
    "\n",
    "where **X** - m by j input matrix, **y** - m by 1 target vector, * - [dot multiplication](https://numpy.org/doc/stable/reference/generated/numpy.dot.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 4. \n",
    "\n",
    "Complete the code to compute the predicted label, error, and gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return the total error and the gradient\n",
    "def compute_cost(theta, x, y):\n",
    "    m = len(y)\n",
    "    # aka h_theta(x^i)\n",
    "    y_pred = None \n",
    "    # The part of J(theta) under the sum\n",
    "    error = None \n",
    "    # J(theta)\n",
    "    cost = -1 / m * sum(error)\n",
    "    # Compute in a vector form \n",
    "    gradient = None \n",
    "    return cost[0] , gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot the data with decision boundary\n",
    "def plot_data_with_decision(data, labels, theta):\n",
    "    colors = {0:\"red\", 1:'blue'}\n",
    "    label_values = [0, 1]\n",
    "\n",
    "    x_boundary = np.array([np.min(data[:, 1]), np.max(data[:, 1])])\n",
    "    y_boundary = -(theta[0] + theta[1] * x_boundary) / theta[2]\n",
    "\n",
    "    for i in label_values:\n",
    "        idx = np.where(labels == i)\n",
    "        plt.scatter(data[idx, 0], data[idx, 1], c=colors[i])\n",
    "    plt.plot(x_boundary, y_boundary)\n",
    "    plt.xlim([-2, 6])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the parameters of the hypothesis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_feature = data.shape[1]\n",
    "n_samples = data.shape[0]\n",
    "\n",
    "X = np.append(np.ones((n_samples, 1)), data, axis=1) #include intercept\n",
    "y = labels.reshape(n_samples, 1)\n",
    "\n",
    "theta = np.random.randn(n_feature+1).reshape(n_feature+1, 1) # initialize the weights (parameters)\n",
    " \n",
    "print(f\"Initial values of theta : {theta}\")\n",
    "cost, gradient = compute_cost(theta, X, y)\n",
    "print(f\"Initial gradient : {gradient}\\nInitial cost {cost}\")\n",
    "\n",
    "plot_data_with_decision(data,labels, theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply gradient descent to minimize the loss function\n",
    "\n",
    "Minimize the cost function $J(\\theta)$ by updating the below equation and repeat until convergence\n",
    "$\\theta_j := \\theta_j - \\alpha \\frac{\\partial J(\\theta)}{\\partial \\theta_j}$ (simultaneously update $\\theta_j$ for all $j$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "costs = []\n",
    "iterations = 300\n",
    "lr = 0.01 # the learning rate\n",
    "for i in range(iterations):  \n",
    "    cost, gradient = compute_cost(theta, X, y)\n",
    "    theta -= (lr * gradient) # simultaneously update the parameters with thier respective gradients\n",
    "    costs.append(cost)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the Convergence of $J(\\theta)$\n",
    "Plot $J(\\theta)$ against the number of iterations of gradient descent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(costs)\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"$J(\\Theta)$\")\n",
    "plt.title(\"Values of Cost Function over iterations of Gradient Descent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the final decision boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = {0:\"red\", 1:'blue'}\n",
    "label_values = [0, 1]\n",
    "\n",
    "x_boundary = np.array([np.min(X[:, 1]), np.max(X[:, 1])])\n",
    "y_boundary = -(theta[0] + theta[1] * x_boundary) / theta[2]\n",
    "\n",
    "for i in label_values:\n",
    "    idx = np.where(labels == i)\n",
    "    plt.scatter(data[idx, 0], data[idx, 1], c=colors[i])\n",
    "plt.plot(x_boundary, y_boundary)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(theta, X):\n",
    "    results = X.dot(theta)\n",
    "    return results > 0.5\n",
    "\n",
    "print(f\"Accuracy: {metrics.accuracy_score(predict(theta, X), y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further reading\n",
    "\n",
    "Classification for more than two classes may be solved by One-vs-Rest or One-vs-One strategies. [link](https://machinelearningmastery.com/one-vs-rest-and-one-vs-one-for-multi-class-classification/)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Week3BSWithAnswers.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
